{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1sXm2wt519o"
      },
      "source": [
        "# Programming Problem 1: HMM for NER (30 points)\n",
        "Welcome to the programming portion of the assignment! Each assignment throughout the semester will have a written portion and a programming portion. We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
        "\n",
        "### Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to add and delete arguments in function signatures, but be careful that you might need to change them in function calls which are already present in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEyk2ch1GYi0"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "In this section we will write code to load data and build the dataset for Named Entity Recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X_hx0PKdII9C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmssMqR2IKkC"
      },
      "source": [
        "Write a function to load sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q4MKd94oHDKr"
      },
      "outputs": [],
      "source": [
        "def zero_digits(s):\n",
        "    \"\"\"\n",
        "    Replace all digits in a string with zeros.\n",
        "    \"\"\"\n",
        "    return re.sub('\\d', '0', s)\n",
        "\n",
        "def load_sentences(path):\n",
        "    \"\"\"\n",
        "    Load sentences. A line must contain at least a word and its tag.\n",
        "    Sentences are separated by empty lines.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in codecs.open(path, 'r', 'utf8'):\n",
        "        line = zero_digits(line.rstrip())\n",
        "        if not line:\n",
        "            if len(sentence) > 0:\n",
        "                if 'DOCSTART' not in sentence[0][0]:\n",
        "                    sentences.append(sentence)\n",
        "                sentence = []\n",
        "        else:\n",
        "            word = line.split()\n",
        "            assert len(word) >= 2\n",
        "            sentence.append(word)\n",
        "    if len(sentence) > 0:\n",
        "        if 'DOCSTART' not in sentence[0][0]:\n",
        "            sentences.append(sentence)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W76Clg4J2wo"
      },
      "source": [
        "Prepare the training/test data using the loaded sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1yS9JsVSMZ6l"
      },
      "outputs": [],
      "source": [
        "def create_dico(item_list):\n",
        "    \"\"\"\n",
        "    Create a dictionary of items from a list of list of items.\n",
        "    \"\"\"\n",
        "    assert type(item_list) is list\n",
        "    dico = {}\n",
        "    for items in item_list:\n",
        "        for item in items:\n",
        "            if item not in dico:\n",
        "                dico[item] = 1\n",
        "            else:\n",
        "                dico[item] += 1\n",
        "    return dico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D_7t9tVCMdFY"
      },
      "outputs": [],
      "source": [
        "def create_mapping(dico):\n",
        "    \"\"\"\n",
        "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
        "    Items are ordered by decreasing frequency.\n",
        "    \"\"\"\n",
        "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
        "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items) if v[1] > 2}\n",
        "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
        "    return item_to_id, id_to_item\n",
        "\n",
        "def word_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of words, sorted by frequency.\n",
        "    \"\"\"\n",
        "    words = [[x[0] for x in s] for s in sentences]\n",
        "    dico = create_dico(words)\n",
        "    dico['<UNK>'] = 10000000\n",
        "    word_to_id, id_to_word = create_mapping(dico)\n",
        "    print(\"Found %i unique words (%i in total)\" % (\n",
        "        len(dico), sum(len(x) for x in words))\n",
        "    )\n",
        "    return dico, word_to_id, id_to_word\n",
        "\n",
        "def tag_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
        "    \"\"\"\n",
        "    tags = [[word[-1] for word in s] for s in sentences]\n",
        "    dico = create_dico(tags)\n",
        "    tag_to_id, id_to_tag = create_mapping(dico)\n",
        "    print(\"Found %i unique named entity tags\" % len(dico))\n",
        "    return dico, tag_to_id, id_to_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lNS57L87IQdT"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(sentences, mode=None, word_to_id=None, tag_to_id=None):\n",
        "    \"\"\"\n",
        "    Prepare the dataset. Return 'data', which is a list of lists of dictionaries containing:\n",
        "        - words (strings)\n",
        "        - word indexes\n",
        "        - tag indexes\n",
        "    \"\"\"\n",
        "    assert mode == 'train' or (mode == 'test' and word_to_id and tag_to_id)\n",
        "\n",
        "    if mode=='train':\n",
        "        word_dic, word_to_id, id_to_word = word_mapping(sentences)\n",
        "        tag_dic, tag_to_id, id_to_tag = tag_mapping(sentences)\n",
        "\n",
        "    def f(x): return x\n",
        "    data = []\n",
        "    for s in sentences:\n",
        "        str_words = [w[0] for w in s]\n",
        "        words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>']\n",
        "                 for w in str_words]\n",
        "        tags = [tag_to_id[w[-1]] for w in s]\n",
        "        data.append({\n",
        "            'str_words': str_words,\n",
        "            'words': words,\n",
        "            'tags': tags,\n",
        "        })\n",
        "\n",
        "    if mode == 'train':\n",
        "        return data, {'word_to_id':word_to_id, 'id_to_word':id_to_word, 'tag_to_id':tag_to_id, 'id_to_tag':id_to_tag}\n",
        "    else:\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgikvX-qKTL7"
      },
      "source": [
        "### Hidden Markov Model\n",
        "In this section, we will implement a bigram hidden markov model (HMM) that could perform two types of decoding: greedy decoding and viterbi decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "te89fFIgLdFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import sparse\n",
        "import collections\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QCiZJCYKMln5"
      },
      "outputs": [],
      "source": [
        "class HMM(object):\n",
        "    \"\"\"\n",
        "     HMM Model\n",
        "    \"\"\"\n",
        "    def __init__(self, dic, decode_type):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_words = len(dic['word_to_id'])\n",
        "        self.num_tags = len(dic['tag_to_id'])\n",
        "\n",
        "        self.initial_prob = np.ones([self.num_tags])\n",
        "        self.transition_prob = np.ones([self.num_tags, self.num_tags])\n",
        "        self.emission_prob = np.ones([self.num_tags, self.num_words])\n",
        "        self.decode_type = decode_type\n",
        "\n",
        "        return\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        TODO: Train a bigram HMM model using MLE estimates.\n",
        "        Complete the code to compute self,initial_prob, self.transition_prob and self.emission_prob appropriately.\n",
        "\n",
        "        Args:\n",
        "            corpus is a list of dictionaries of the form:\n",
        "            {'str_words': str_words,   ### List of string words\n",
        "            'words': words,            ### List of word IDs\n",
        "            'tags': tags}              ### List of tag IDs\n",
        "            All three lists above have length equal to the sentence length for each instance.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Compute initial_probs.\n",
        "        # initial_prob[x]: the probability of tag x to be assigned to the first token in a sentence.\n",
        "        self.initial_prob = np.zeros([self.num_tags])\n",
        "        for sentence in corpus:\n",
        "            # TODO: update self.initial_prob\n",
        "            # (5 points)\n",
        "            # START HERE\n",
        "            first_tag = sentence['tags'][0]#get tag ID of the first word in the sentence.\n",
        "            self.initial_prob[first_tag] += 1\n",
        "            # END HERE\n",
        "\n",
        "        # Normarlize initial_prob to sum to 1\n",
        "        self.initial_prob /= np.sum(self.initial_prob)\n",
        "\n",
        "        # Step 2: Complete the code to compute transition_prob.\n",
        "        # transition_prob[x][y]: the probability of tag y to appear after tag x.\n",
        "        self.transition_prob = np.zeros([self.num_tags, self.num_tags])\n",
        "        for sentence in corpus:\n",
        "            tag = sentence['tags']\n",
        "            for i in range(1, len(tag)):\n",
        "                # TODO: update self.transition_prob\n",
        "                # (5 points)\n",
        "                # START HERE\n",
        "                self.transition_prob[tag[i-1]][tag[i]]+=1 #i given i-1\n",
        "                # END HERE\n",
        "\n",
        "        # Normalize every row of transition_prob to sum to 1.\n",
        "        for p in self.transition_prob:\n",
        "            p /= np.sum(p)\n",
        "\n",
        "\n",
        "        # Step 3: Complete the code to compute emission_prob\n",
        "        # emission_prob[x][y]: the probability of word y to appear given tag x.\n",
        "        # Note that for each sentence s in the corpus, word IDs are in s['words'].\n",
        "        self.emission_prob = np.zeros([self.num_tags, self.num_words])\n",
        "        for sentence in corpus:\n",
        "            for i in range(len(sentence['tags'])):\n",
        "                # TODO: update self.emission_prob\n",
        "                # (5 points)\n",
        "                # START HERE\n",
        "                self.emission_prob[sentence['tags'][i]][sentence['words'][i]]+=1 #word given tag\n",
        "                # END HERE\n",
        "\n",
        "        # For every tag, normalize the emission_prob to sum to 1.\n",
        "        for p in self.emission_prob:\n",
        "            p /= np.sum(p)\n",
        "\n",
        "        return\n",
        "\n",
        "    def greedy_decode(self, sentence):\n",
        "        \"\"\"\n",
        "        Decode a single sentence in Greedy fashion\n",
        "        Return a list of tags.\n",
        "        \"\"\"\n",
        "        tags = []\n",
        "\n",
        "        init_scores = [self.initial_prob[t] * self.emission_prob[t][sentence[0]] for t in range(self.num_tags)]\n",
        "        tags.append(np.argmax(init_scores))\n",
        "\n",
        "        for w in sentence[1:]:\n",
        "            scores = [self.transition_prob[tags[-1]][t] * self.emission_prob[t][w] for t in range(self.num_tags)]\n",
        "            tags.append(np.argmax(scores))\n",
        "\n",
        "        assert len(tags) == len(sentence)\n",
        "        return tags\n",
        "\n",
        "    def viterbi_decode(self, sentence):\n",
        "        \"\"\"\n",
        "        TODO: Complete the code to decode a single sentence using the Viterbi algorithm.\n",
        "        Args:\n",
        "             sentence -- a list of ints that represents word IDs.\n",
        "        Output:\n",
        "             tags     -- a list of ints that represents the tags decoded from the input.\n",
        "        \"\"\"\n",
        "        tags = []\n",
        "        l = len(sentence)\n",
        "\n",
        "        pi = self.initial_prob\n",
        "        A = self.transition_prob\n",
        "        O = self.emission_prob\n",
        "\n",
        "        # Let M be the state matrix.\n",
        "        # M[i,j]: most probable sequence of tags ending with tag j at the i-th token.\n",
        "        M = np.zeros((l, self.num_tags))\n",
        "        M[:,:] = float('-inf')\n",
        "\n",
        "        # Use B to track the path to reach the most probable sequence.\n",
        "        # B[i,j] is the tag of the (i-1)-th token in the most probable sequence ending with tag j at the i-th token.\n",
        "        B = np.zeros((l, self.num_tags), 'int')\n",
        "\n",
        "        # Compute the probability to assign each tag to the first token.\n",
        "        M[0, :] = pi * O[:, sentence[0]]\n",
        "\n",
        "        # Dynamic programming.\n",
        "        for i in range(1, l):\n",
        "            for j in range(self.num_tags):\n",
        "                # TODO: Compute M[i, j] and B[i, j].\n",
        "                # (10 points)\n",
        "                # START HERE\n",
        "                max_prob = float('-inf')\n",
        "                best_prev_tag = 0\n",
        "\n",
        "                for prev_tag in range(self.num_tags):\n",
        "                    prob = M[i-1, prev_tag] * A[prev_tag, j] * O[j, sentence[i]]\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        best_prev_tag = prev_tag\n",
        "\n",
        "                M[i, j] = max_prob\n",
        "                B[i, j] = best_prev_tag\n",
        "                # END HERE\n",
        "\n",
        "        #print(B)\n",
        "        # Extract the optimal sequence of tags from B.\n",
        "        # Start from the last position, and iteratively find the tag of each position that results in the most probable tag sequence.\n",
        "        tags.append(np.argmax(M[l-1,:]))\n",
        "        most_probable=np.argmax(M[l-1,:])\n",
        "        for i in range(l-1, 0, -1):\n",
        "            # TODO: Extract the tag of the (i-1)-th token that results in the most probable sequence of tags.\n",
        "            # (5 points)\n",
        "            # START HERE\n",
        "            most_probable= B[i,most_probable]\n",
        "            tags.append(most_probable)\n",
        "        tags.reverse()\n",
        "            # END HERE\n",
        "\n",
        "        return tags\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        \"\"\"\n",
        "        Tag a sentence using a trained HMM.\n",
        "        \"\"\"\n",
        "        if self.decode_type == 'viterbi':\n",
        "            return self.viterbi_decode(sentence)\n",
        "        elif self.decode_type == 'greedy':\n",
        "            return self.greedy_decode(sentence)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown decoding type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6P0Qs5JM0x3"
      },
      "source": [
        "### Train and evaluate HMMs.\n",
        "This section contains driver code for learning a HMM for named entity recognition on a training corpus and evaluating it on a test corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DJRRLA2jOsl1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.metrics import f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk2KF59LOzSr"
      },
      "source": [
        "Write a function to tag the test corpus with a trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ldGSAHWSOwt1"
      },
      "outputs": [],
      "source": [
        "def tag_corpus(model, test_corpus, output_file, dic):\n",
        "    if output_file:\n",
        "        f_output = codecs.open(output_file, 'w', 'utf-8')\n",
        "    start = time.time()\n",
        "\n",
        "    num_correct = 0.\n",
        "    num_total = 0.\n",
        "    y_pred=[]\n",
        "    y_actual=[]\n",
        "    print('Tagging...')\n",
        "    for i, sentence in enumerate(tqdm(test_corpus)):\n",
        "        tags = model.tag(sentence['words'])\n",
        "        str_tags = [dic['id_to_tag'][t] for t in tags]\n",
        "        y_pred.extend(tags)\n",
        "        y_actual.extend(sentence['tags'])\n",
        "\n",
        "        # Check accuracy.\n",
        "        num_correct += np.sum(np.array(tags) == np.array(sentence['tags']))\n",
        "        num_total += len([w for w in sentence['words']])\n",
        "\n",
        "        if output_file:\n",
        "            f_output.write('%s\\n' % ' '.join('%s%s%s' % (w, '__', y)\n",
        "                                             for w, y in zip(sentence['str_words'], str_tags)))\n",
        "\n",
        "    print('---- %i lines tagged in %.4fs ----' % (len(test_corpus), time.time() - start))\n",
        "    if output_file:\n",
        "        f_output.close()\n",
        "\n",
        "    print(\"Overall accuracy: %s\\n\" % (num_correct/num_total))\n",
        "    return y_pred,y_actual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhAjb7OuPEgT"
      },
      "source": [
        "Write a function a compute the confusion matrix and the F-1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KC_VhUG0O98M"
      },
      "outputs": [],
      "source": [
        "def compute_score(y_pred,y_actual):\n",
        "    A = confusion_matrix(y_actual, y_pred)\n",
        "    f1 = f1_score(y_actual, y_pred,average=None)\n",
        "    print(\"Confusion Matrix:\\n\", A)\n",
        "    print(\"F-1 scores: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLFBiD4BPMl1"
      },
      "source": [
        "Write a function to train and evalute the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RFcYYThhPRbh"
      },
      "outputs": [],
      "source": [
        "def runHiddenMarkovModel(train_corpus,\n",
        "                         test_corpus,\n",
        "                         dic,\n",
        "                         decode_type,\n",
        "                         output_file):\n",
        "    # build and train the model\n",
        "    model = HMM(dic, decode_type)\n",
        "    model.train(train_corpus)\n",
        "\n",
        "    print(\"Train results:\")\n",
        "    pred, real = tag_corpus(model, train_corpus, output_file, dic)\n",
        "\n",
        "    print(\"Tags: \", dic['id_to_tag'])\n",
        "    A = compute_score(pred,real)\n",
        "\n",
        "    # test on validation\n",
        "    print(\"\\n-----------\\nTest results:\")\n",
        "    pred, real = tag_corpus(model, test_corpus, output_file, dic)\n",
        "\n",
        "    print(\"Tags: \", dic['id_to_tag'])\n",
        "    A = compute_score(pred,real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vRsc-YrRAb2"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McZ9KpeQRlA9"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mjVwUkR9Rq53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20101 unique words (203621 in total)\n",
            "Found 5 unique named entity tags\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "# !wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "# !wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "\n",
        "train_file = 'eng.train'\n",
        "test_file = 'eng.val'\n",
        "\n",
        "# Load the training data\n",
        "train_sentences = load_sentences(train_file)\n",
        "train_corpus, dic = prepare_dataset(train_sentences, mode='train', word_to_id=None, tag_to_id=None)\n",
        "\n",
        "# Load the testing data\n",
        "test_sentences = load_sentences(test_file)\n",
        "test_corpus = prepare_dataset(test_sentences, mode='test', word_to_id=dic['word_to_id'], tag_to_id=dic['tag_to_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzNnUiBZS4ME"
      },
      "source": [
        "#### Experiment with an HMM with greedy decoding for Problem 2(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h0t7W9JMTfLl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train results:\n",
            "Tagging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14041/14041 [00:01<00:00, 11170.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- 14041 lines tagged in 1.2600s ----\n",
            "Overall accuracy: 0.9544742438157164\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[168060    135   1233     48    102]\n",
            " [  1999   8628    456     39      6]\n",
            " [  1632     98   7291    731    273]\n",
            " [   741     49    567   6886     54]\n",
            " [   665     32    206    204   3486]]\n",
            "F-1 scores:  [0.98087109 0.85979073 0.73728385 0.84986115 0.81888654]\n",
            "\n",
            "-----------\n",
            "Test results:\n",
            "Tagging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3490/3490 [00:00<00:00, 10480.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- 3490 lines tagged in 0.3350s ----\n",
            "Overall accuracy: 0.9241331540561464\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[40558    17   543    17    29]\n",
            " [ 1105  1294   267    16     8]\n",
            " [  606    30  1382   176    56]\n",
            " [  249    15   215  1478    18]\n",
            " [  233     8    79    37   650]]\n",
            "F-1 scores:  [0.96664482 0.63838185 0.58361486 0.7991349  0.73529412]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "runHiddenMarkovModel(\n",
        "    train_corpus = train_corpus,\n",
        "    test_corpus = test_corpus,\n",
        "    dic = dic,\n",
        "    decode_type = 'greedy',\n",
        "    output_file = None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP4iMShYX3Xp"
      },
      "source": [
        "#### Experiment with an HMM with viterbi decoding for Problem 2(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_zBEh-TpX8KC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train results:\n",
            "Tagging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14041/14041 [00:02<00:00, 5383.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- 14041 lines tagged in 2.6092s ----\n",
            "Overall accuracy: 0.9633878627450018\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[168181    593    581     51    172]\n",
            " [  1693   9323     79     29      4]\n",
            " [  1267    331   7944    379    104]\n",
            " [   890     90    341   6937     39]\n",
            " [   600     55     96     61   3781]]\n",
            "F-1 scores:  [0.98291395 0.86644981 0.83331585 0.88066523 0.86989532]\n",
            "\n",
            "-----------\n",
            "Test results:\n",
            "Tagging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3490/3490 [00:00<00:00, 5592.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- 3490 lines tagged in 0.6260s ----\n",
            "Overall accuracy: 0.9315079656113759\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[40591   346   150    22    55]\n",
            " [ 1099  1531    42    11     7]\n",
            " [  563   105  1412   135    35]\n",
            " [  317    47   103  1491    17]\n",
            " [  223    30    42    13   699]]\n",
            "F-1 scores:  [0.96694737 0.64476732 0.70617654 0.81765835 0.76813187]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "runHiddenMarkovModel(\n",
        "    train_corpus = train_corpus,\n",
        "    test_corpus = test_corpus,\n",
        "    dic = dic,\n",
        "    decode_type = 'viterbi',\n",
        "    output_file = None\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
